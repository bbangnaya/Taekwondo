{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "295a7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. module import\n",
    "from ast import increment_lineno\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e2ebde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 딥러닝 모델 설계할 때 활용하는 장비 확인\n",
    "if torch.cuda.is_available():\n",
    "     DEVICE=torch.device('cuda')\n",
    "else:\n",
    "     DEVICE=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b0b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42b3d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.Resize((128,128)),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "                            ])\n",
    "train_dataset = torchvision.datasets.ImageFolder(root = \"./Taekwondo/DataSet/train_Image/\",\n",
    "                                                 transform = trans)\n",
    "                                                 \n",
    "train_loader = DataLoader(train_dataset, \n",
    "                        batch_size = 36,\n",
    "                        shuffle = False,\n",
    "                        num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2028532",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = transforms.Compose([transforms.Resize((128,128)),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "                            ])\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=\"./Taekwondo/DataSet/test_Image/\",\n",
    "                                                transform=tests)\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                        batch_size = 36,\n",
    "                        shuffle = False,\n",
    "                        num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83ef0cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([36, 3, 128, 128]) type: torch.FloatTensor\n",
      "Y_train: torch.Size([36]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for (X_train,Y_train) in train_loader:\n",
    "        print('X_train:',X_train.size(),'type:', X_train.type())\n",
    "        print('Y_train:',Y_train.size(),'type:', Y_train.type())\n",
    "        break\n",
    "\n",
    "        pltsize = 1\n",
    "        plt.figure(figsize=(10 * pltsize, pltsize))\n",
    "\n",
    "        for i in range(10):\n",
    "            plt.subplot(1,10,i+1)     # 여러 그래프 그리기, 첫숫자 : 행, 둘째 : 열\n",
    "            plt.axis('off')           # 축없음\n",
    "            plt.imshow(np.transpose(X_train[i],(1,2,0)))\n",
    "            plt.title('Class: ' + str(Y_train[i].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc2ee910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):                 # nn.Module을 상속받는 Net클래스 생성\n",
    "    def __init__(self):   \n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, \n",
    "                               out_channels = 16, \n",
    "                               kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, \n",
    "                               out_channels = 32, \n",
    "                               kernel_size = 3)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, \n",
    "                               out_channels = 64, \n",
    "                               kernel_size = 3)\n",
    "        self.conv4 = nn.Conv2d(in_channels = 64, \n",
    "                               out_channels = 128, \n",
    "                               kernel_size = 3)\n",
    "        # 2차원 이미지 데이터를 nn.Conv2d메서드를 이용해 Convolution연산을 하는 filter를 정의\n",
    "        \n",
    "        # in_channels = 3\n",
    "        # 채널 수를 이미지의 채널수와 맞춰야 한다. RGB 채널은 채널수 = 3 이다.\n",
    "        \n",
    "        # out_channels = 8\n",
    "        # Convolution연산을 진행하는 필터 개수\n",
    "        # 여기서 설정해주는 Filter개수만큼 Output의 depth가 정해집니다.\n",
    "        # 여기서는 depth가 8인 Feature Map이 생성\n",
    "        \n",
    "        # kernel_size = 3\n",
    "        # Filter의 크기. 스칼라 값으로 설정하려면 가로 * 세로 크기인 Filter를 이용.\n",
    "        # kernel_size = 3이면 3*3 의 필터가 이미지 위를 돌아다니면서 겹치는 영역에 대해\n",
    "        # 9개의 픽셀 값과  Filter내에 있는 9개의 파라미터 값을 Convolution연산으로 진행.\n",
    "    \n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2,\n",
    "                                 stride = 2)\n",
    "        self.fc1=nn.Linear(1024,128) # (input node수, output node수)\n",
    "        self.fc2=nn.Linear(128,8)   # 이전 output node수 = 다음 input node수\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(-1, 8 * 8 * 16)\n",
    "        # 최종 생성된 Feature Map 의 모양은 8*8*16 크기이므로 이를 펼치기 위해 view 함수를 이용해 \n",
    "        # Feature Map을 1차원 데이터로 변환\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf6bf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    model=CNN().to(DEVICE)                  # MLP모델을 기존에 선정한 'DEVICE'에 할당합니다. \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "# Back Propagation을 이용해 파라미터를 업데이트할 때 이용하는 Optimizer를 정의합니다.\n",
    "# SGD알고리즘을 이용하며 Learning Rate = 0.01, momentum=0.5로 설정\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "# MLP모델의 output값과 계산될 Label값은 Class를 표현하는 원-핫 인코딩 값입니다.\n",
    "# MLP모델의 output값과 원-핫 인코딩 값과의 Loss는 CrossEntropy를 이용해 계산하기 위해\n",
    "# criterion은 nn.CrossEntropyLoss() 로 설정. \n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7df00fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()         # 학습상태로 지정\n",
    "    for batch_idx,(image, label) in enumerate(train_loader):\n",
    "        image=image.to(DEVICE)\n",
    "        label=label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(image)\n",
    "        loss=criterion(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval==0:\n",
    "            print(\"Train Epoch: {}[{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}\"\n",
    "                  .format(Epoch,batch_idx * len(image),\n",
    "                len(train_loader.dataset),100.*batch_idx/len(train_loader),loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2872ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,test_loader):\n",
    "    model.eval()                  # 평가상태로 지정\n",
    "    test_loss=0\n",
    "    correct=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image=image.to(DEVICE)              # 8과 동일\n",
    "            label=label.to(DEVICE)              # 8과 동일\n",
    "            output=model(image)                 # 8과 동일\n",
    "            test_loss+=criterion(output,label).item()\n",
    "            prediction = output.max(1,keepdim = True)[1]\n",
    "            # MLP 모델의 output값은 크기가 10인 벡터값입니다. \n",
    "            # 계산된 벡터값 내 가장 큰 값인 위치에 대해 해당 위치에 대응하는 클래스로 예측했다고 판단합니다.\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            # MLP모델이 최종으로 예측한 클래스 값과 실제 레이블이 의미하는 클래스가 맞으면 correct에 더해 올바르게 예측한 횟수를 저장\n",
    "                        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    # 현재까지 계산된 test_loss 값을 test_loader내에 존재하는 Mini-Batch 개수(=10)만큼 나눠 평균 Loss값으로 계산.\n",
    "    test_accuracy=100.*correct / len(test_loader.dataset)\n",
    "    # test_loader 데이터중 얼마나 맞췄는지를 계산해 정확도를 계산합니다.\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d061cf96",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (162) to match target batch_size (36).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m         test_loss, test_accuracy\u001b[38;5;241m=\u001b[39mevaluate(model, test_loader)\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[EPOCH: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m], \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;28mformat\u001b[39m(Epoch,test_loss,test_accuracy))\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, log_interval)\u001b[0m\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      7\u001b[0m output\u001b[38;5;241m=\u001b[39mmodel(image)\n\u001b[1;32m----> 8\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\project\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\project\\lib\\site-packages\\torch\\nn\\functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2995\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (162) to match target batch_size (36)."
     ]
    }
   ],
   "source": [
    "for Epoch in range(1,EPOCHS+1):\n",
    "    if __name__ == \"__main__\":\n",
    "        train(model,train_loader,optimizer,log_interval = 200)\n",
    "        test_loss, test_accuracy=evaluate(model, test_loader)\n",
    "        print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n\".\n",
    "            format(Epoch,test_loss,test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a66683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae3dce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
